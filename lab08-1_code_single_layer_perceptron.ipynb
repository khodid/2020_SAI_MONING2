{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5VLsC7a4UR/Lo81qd53rG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khodid/2020_SAI_MONING2/blob/master/lab08-1_code_single_layer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf8E78QipIEY",
        "colab_type": "text"
      },
      "source": [
        "# 단일 레이어 퍼셉트론으로 XOR 문제 시도하기\n",
        "\n",
        "XOR 문제를 PyTorch로 모델링하고,\n",
        "\n",
        "단일 레이어로는 XOR 구조를 해결하지 못한다는 것을 확실하게 관찰해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqtl8mdRlOGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "\n",
        "# Train Data\n",
        "\n",
        "X = torch.FloatTensor([[0,0], [0,1],[1,0],[1,1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mef_Fw39pikT",
        "colab_type": "text"
      },
      "source": [
        "다음은 학습 모델이다.\n",
        "nn 패키지를 이용하여 레이어를 구성해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFPsfrS4pmoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단일 perceptron 선언 (1개 layer)\n",
        "linear = torch.nn.Linear(2,1, bias = True)    # 2 inputs, 1 output\n",
        "sigmoid = torch.nn.Sigmoid()   \n",
        "model = torch.nn.Sequential(linear, sigmoid).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo3jWi8wp_9m",
        "colab_type": "text"
      },
      "source": [
        "예전에 하던 것과 동일한 방식으로 학습 모델을 만들어준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDJUYa9PqLol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfc9c282-4b5f-4646-b7c8-f0d59fe2b7a8"
      },
      "source": [
        "criterion = torch.nn.BCELoss().to(device)     # Binary Classification이기 때문에 BCE로 loss 계산\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(X)\n",
        "\n",
        "  cost = criterion(hypothesis, Y)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print (step, cost.item())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7666423320770264\n",
            "100 0.6931473016738892\n",
            "200 0.6931471824645996\n",
            "300 0.6931471824645996\n",
            "400 0.6931471824645996\n",
            "500 0.6931471824645996\n",
            "600 0.6931471824645996\n",
            "700 0.6931471824645996\n",
            "800 0.6931471824645996\n",
            "900 0.6931471824645996\n",
            "1000 0.6931471824645996\n",
            "1100 0.6931471824645996\n",
            "1200 0.6931471824645996\n",
            "1300 0.6931471824645996\n",
            "1400 0.6931471824645996\n",
            "1500 0.6931471824645996\n",
            "1600 0.6931471824645996\n",
            "1700 0.6931471824645996\n",
            "1800 0.6931471824645996\n",
            "1900 0.6931471824645996\n",
            "2000 0.6931471824645996\n",
            "2100 0.6931471824645996\n",
            "2200 0.6931471824645996\n",
            "2300 0.6931471824645996\n",
            "2400 0.6931471824645996\n",
            "2500 0.6931471824645996\n",
            "2600 0.6931471824645996\n",
            "2700 0.6931471824645996\n",
            "2800 0.6931471824645996\n",
            "2900 0.6931471824645996\n",
            "3000 0.6931471824645996\n",
            "3100 0.6931471824645996\n",
            "3200 0.6931471824645996\n",
            "3300 0.6931471824645996\n",
            "3400 0.6931471824645996\n",
            "3500 0.6931471824645996\n",
            "3600 0.6931471824645996\n",
            "3700 0.6931471824645996\n",
            "3800 0.6931471824645996\n",
            "3900 0.6931471824645996\n",
            "4000 0.6931471824645996\n",
            "4100 0.6931471824645996\n",
            "4200 0.6931471824645996\n",
            "4300 0.6931471824645996\n",
            "4400 0.6931471824645996\n",
            "4500 0.6931471824645996\n",
            "4600 0.6931471824645996\n",
            "4700 0.6931471824645996\n",
            "4800 0.6931471824645996\n",
            "4900 0.6931471824645996\n",
            "5000 0.6931471824645996\n",
            "5100 0.6931471824645996\n",
            "5200 0.6931471824645996\n",
            "5300 0.6931471824645996\n",
            "5400 0.6931471824645996\n",
            "5500 0.6931471824645996\n",
            "5600 0.6931471824645996\n",
            "5700 0.6931471824645996\n",
            "5800 0.6931471824645996\n",
            "5900 0.6931471824645996\n",
            "6000 0.6931471824645996\n",
            "6100 0.6931471824645996\n",
            "6200 0.6931471824645996\n",
            "6300 0.6931471824645996\n",
            "6400 0.6931471824645996\n",
            "6500 0.6931471824645996\n",
            "6600 0.6931471824645996\n",
            "6700 0.6931471824645996\n",
            "6800 0.6931471824645996\n",
            "6900 0.6931471824645996\n",
            "7000 0.6931471824645996\n",
            "7100 0.6931471824645996\n",
            "7200 0.6931471824645996\n",
            "7300 0.6931471824645996\n",
            "7400 0.6931471824645996\n",
            "7500 0.6931471824645996\n",
            "7600 0.6931471824645996\n",
            "7700 0.6931471824645996\n",
            "7800 0.6931471824645996\n",
            "7900 0.6931471824645996\n",
            "8000 0.6931471824645996\n",
            "8100 0.6931471824645996\n",
            "8200 0.6931471824645996\n",
            "8300 0.6931471824645996\n",
            "8400 0.6931471824645996\n",
            "8500 0.6931471824645996\n",
            "8600 0.6931471824645996\n",
            "8700 0.6931471824645996\n",
            "8800 0.6931471824645996\n",
            "8900 0.6931471824645996\n",
            "9000 0.6931471824645996\n",
            "9100 0.6931471824645996\n",
            "9200 0.6931471824645996\n",
            "9300 0.6931471824645996\n",
            "9400 0.6931471824645996\n",
            "9500 0.6931471824645996\n",
            "9600 0.6931471824645996\n",
            "9700 0.6931471824645996\n",
            "9800 0.6931471824645996\n",
            "9900 0.6931471824645996\n",
            "10000 0.6931471824645996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KYmYVUxsMSs",
        "colab_type": "text"
      },
      "source": [
        "위 출력 결과를 보면, 초반을 제외하곤 cost 값이 아예 줄어들지 않는 것을 확인할 수 있다.\n",
        "학습이 제대로 안 된다는 증거!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kol2bZGLtEox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f75ab043-555f-4ff0-b5b7-1bfd4c1ace08"
      },
      "source": [
        "# 강의 깃헙에서 퍼옴\n",
        "with torch.no_grad():\n",
        "    hypothesis = model(X)\n",
        "    predicted = (hypothesis > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hypothesis:  [[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]] \n",
            "Correct:  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]] \n",
            "Accuracy:  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdRQiRKxtD7K",
        "colab_type": "text"
      },
      "source": [
        "위 코드는 [모두의 딥러닝 시즌 2 lab08_1](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-08_1_xor.ipynb) 에서 퍼왔다.\n",
        "결과를 보면 예측은 전부 0이나 1이 아닌 0.5로 하고 있고, 정확도도 딱 0.5 (50%) 인 것을 확인할 수 있다."
      ]
    }
  ]
}