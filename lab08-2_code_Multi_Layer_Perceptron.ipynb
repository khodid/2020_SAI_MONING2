{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9qcptaDVJJ4jbcbD25Y4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khodid/2020_SAI_MONING2/blob/master/lab08-1_code_Multi_Layer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leAz2x35u_WL",
        "colab_type": "text"
      },
      "source": [
        "# Multi Layer Perceptron\n",
        "\n",
        "이번에는 여러 레이어로 이루어진 모델을 학습시켜, XOR 문제에 대한 제대로 된 결과를 내놓는 걸 관찰하겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsKyXmWkwota",
        "colab_type": "text"
      },
      "source": [
        "## 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNVi7-0Qwy5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylqs_IoIwcl5",
        "colab_type": "text"
      },
      "source": [
        "## Back Propagation 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9RobL4JwiXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Data\n",
        "X = torch.FloatTensor([[0, 0],[0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# layers\n",
        "w1 = torch.Tensor(2, 2).to(device)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2, 1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4UgyCkAxmG7",
        "colab_type": "text"
      },
      "source": [
        "레이어는 이런 식으로 구성되어 있다.\n",
        "\n",
        "$$\n",
        "(X_{train} \\cdot w_1 + b_1)\\cdot w_2 + b_2\n",
        "$$\n",
        "\n",
        "이를 좀더 풀어서 쓰면,\n",
        "여기서 첫번째 레이어는\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x_1 & x_2\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "w_{a1} & w_{b1} \\\\ w_{a2} & w_{b2}\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "b_a & b_b\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "=\n",
        "\\begin{bmatrix} \n",
        "x_1 \\cdot w_{a1} + x_2 \\cdot w_{a2} + b_a  \n",
        "& \n",
        "x_1 \\cdot w_{b1} + x_2 \\cdot w_{b2} + b_b\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "라는 결과에서 정답이 0인지 1인지를 판별하고,\n",
        "두 번째 레이어에서는\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \n",
        "x_{next1}  \n",
        "& \n",
        "x_{next2}  \n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} \n",
        " w_{2a}\n",
        " \\\\\n",
        " w_{2b}\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix} \n",
        "b_{2a}  \n",
        "& \n",
        "b_{2b}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "라는 연산을 해서 결과를 내놓는다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQV0phFr4BZ5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYj9f_Aj4GyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시그모이드도 PyTorch 라이브버리에서 쓰지 않고 직접 짜보겠다.\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0 / (1.0 + torch.exp(-x))\n",
        "\n",
        "# 시그모이드 함수 미분꼴\n",
        "def sigmoid_prime(x):\n",
        "  return sigmoid(x) * (1-sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT66BNk_4eq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "44ec95a1-5901-4f4a-e20f-9db9aa502d57"
      },
      "source": [
        "\n",
        "for step in range(10001):\n",
        "  \n",
        "  # forward\n",
        "\n",
        "  # 위에서 언급한 수식을 l1, l2로 구현.\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  a1 = sigmoid(l1)\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  # Cost는 Binary Cross Entropy\n",
        "  cost = -torch.mean(Y* torch.log(Y_pred) + (1-Y)*torch.log(1-Y_pred))\n",
        "\n",
        "  #Back propagation (chain rule)\n",
        "\n",
        "  # BCE를 미분한 식.\n",
        "  d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7) # 1e-7은 분모가 0 되는 걸 막는 상수\n",
        "\n",
        "  # Layer 2\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
        "  d_b2 = d_l2\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2) \n",
        "\n",
        "  # Layer 1\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "\n",
        "\n",
        "  # Weight update\n",
        "  ## Gradient Descent\n",
        "  w1 = w1 - learning_rate * d_w1\n",
        "  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate * d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if (step % 100 == 0 and step < 2000) or (step % 400 == 0):\n",
        "    print(step, cost.item())\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.6931471824645996\n",
            "100 0.6931471824645996\n",
            "200 0.6931471824645996\n",
            "300 0.6931471824645996\n",
            "400 0.6931471824645996\n",
            "500 0.6931471824645996\n",
            "600 0.6931471228599548\n",
            "700 0.6931452751159668\n",
            "800 0.6930633187294006\n",
            "900 0.6412245035171509\n",
            "1000 0.2531474232673645\n",
            "1100 0.03129035234451294\n",
            "1200 0.016702039167284966\n",
            "1300 0.01122652180492878\n",
            "1400 0.008407686837017536\n",
            "1500 0.006701599806547165\n",
            "1600 0.0055622230283916\n",
            "1700 0.004749101586639881\n",
            "1800 0.0041405013762414455\n",
            "1900 0.0036683687940239906\n",
            "2000 0.003291659988462925\n",
            "2400 0.002328727161511779\n",
            "2800 0.0017987260362133384\n",
            "3200 0.001463932334445417\n",
            "3600 0.001233537564985454\n",
            "4000 0.0010654086945578456\n",
            "4400 0.0009373538196086884\n",
            "4800 0.0008366158581338823\n",
            "5200 0.0007553243776783347\n",
            "5600 0.0006883729947730899\n",
            "6000 0.0006322836852632463\n",
            "6400 0.0005845638224855065\n",
            "6800 0.0005435418570414186\n",
            "7200 0.0005078301182948053\n",
            "7600 0.0004765183839481324\n",
            "8000 0.00044881596113555133\n",
            "8400 0.0004242006689310074\n",
            "8800 0.00040207590791396797\n",
            "9200 0.0003821431891992688\n",
            "9600 0.00036408944288268685\n",
            "10000 0.0003476759302429855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m48fOw0xAhFv",
        "colab_type": "text"
      },
      "source": [
        "cost가 점차 줄어드는 걸 관찰할 수 있다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdAUDeDH-VHS",
        "colab_type": "text"
      },
      "source": [
        "transpose는 전치행렬을 생각하면 된다.\n",
        "뒤에 들어가는 인자는 바꿔줄 차원 방향을 정해주는 것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp-R-rx95Zmg",
        "colab_type": "text"
      },
      "source": [
        "참고로 위의 \n",
        "d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred))\n",
        "의 그래프를 그려보면,\n",
        "Y_pred = 0과 Y_pred = 1을 점근선으로 가지고 Y_pred가 Y값과 일치하는 지점에서 0이 되는 그래프가 나온다.\n",
        "\n",
        "![https://ibb.co/1KmCWGP](https://i.ibb.co/QnKWwvT/dl2.png)\n",
        "\n",
        "대충 이런 느낌임...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGEfusoTA25I",
        "colab_type": "text"
      },
      "source": [
        "## 실전 - nn 패키지 이용\n",
        "\n",
        "아까까진 정말 원론적으로 이론대로 구현시키는 방법이었고,\n",
        "매 번 모델을 만들 때마다 저렇게 길고 수학적인 공식을 만들 수는 없는 법이니 패키지를 사용하는 방법을 알아보도록 하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qxv5AglBI42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "d8ff7361-552e-4794-9398-638dd21e5708"
      },
      "source": [
        "# Training Data\n",
        "X = torch.FloatTensor([[0, 0],[0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "\n",
        "# nn Layers\n",
        "linear1 = torch.nn.Linear(2, 2, bias = True)\n",
        "linear2 = torch.nn.Linear(2, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
        "\n",
        "# cost function과 optimizer 설정\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "# 학습\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(X)\n",
        "\n",
        "  cost = criterion(hypothesis, Y)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (step % 100 == 0 and step < 1000) or (step % 500 ==0):\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7126196026802063\n",
            "100 0.6933621168136597\n",
            "200 0.6932636499404907\n",
            "300 0.6932241916656494\n",
            "400 0.693203866481781\n",
            "500 0.6931912899017334\n",
            "600 0.6931823492050171\n",
            "700 0.6931754350662231\n",
            "800 0.6931697130203247\n",
            "900 0.6931648254394531\n",
            "1000 0.6931604743003845\n",
            "1500 0.6931399703025818\n",
            "2000 0.6930841207504272\n",
            "2500 0.6895962357521057\n",
            "3000 0.04585985466837883\n",
            "3500 0.013895433396100998\n",
            "4000 0.00805296003818512\n",
            "4500 0.005646144971251488\n",
            "5000 0.004339577630162239\n",
            "5500 0.003520903643220663\n",
            "6000 0.002960511948913336\n",
            "6500 0.0025531407445669174\n",
            "7000 0.0022437721490859985\n",
            "7500 0.0020009279251098633\n",
            "8000 0.0018052862724289298\n",
            "8500 0.0016442961059510708\n",
            "9000 0.0015095683047547936\n",
            "9500 0.0013951826840639114\n",
            "10000 0.0012968203518539667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoe_G4KvCdPP",
        "colab_type": "text"
      },
      "source": [
        "1000 이전엔 100 step마다 출력하고, 그 이후론 500마다 출력하게 했다.\n",
        "\n",
        "10000번 이후로도 계속 loss가 감소하는 걸 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBUjFOXGCyyv",
        "colab_type": "text"
      },
      "source": [
        "## nn 패키지로 다층 레이어 쌓아보기\n",
        "\n",
        "방금 전에는 레이어 두 개를 이용했는데, 이번엔 레이어를 좀더 많이 쌓아보는 코드를 살펴보자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DN8Qx7lC9AS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "c67a8a45-eb31-4ca3-f57f-526a4ea07c4f"
      },
      "source": [
        "# Training Data\n",
        "X = torch.FloatTensor([[0, 0],[0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "\n",
        "# nn Layers : build 4 layers\n",
        "linear1 = torch.nn.Linear(2, 10, bias = True)\n",
        "linear2 = torch.nn.Linear(10, 10, bias = True)\n",
        "linear3 = torch.nn.Linear(10, 10, bias = True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "# 모델 설정\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
        "\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (step % 100 == 0 and step < 2000) or (step % 500 ==0):\n",
        "        print(step, cost.item())\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.6937791109085083\n",
            "100 0.6931153535842896\n",
            "200 0.6931127309799194\n",
            "300 0.693109929561615\n",
            "400 0.6931067705154419\n",
            "500 0.6931031942367554\n",
            "600 0.6930992007255554\n",
            "700 0.6930948495864868\n",
            "800 0.6930898427963257\n",
            "900 0.693084180355072\n",
            "1000 0.693077564239502\n",
            "1100 0.6930699348449707\n",
            "1200 0.693061113357544\n",
            "1300 0.6930506229400635\n",
            "1400 0.6930381655693054\n",
            "1500 0.6930232644081116\n",
            "1600 0.6930047869682312\n",
            "1700 0.6929820775985718\n",
            "1800 0.6929532289505005\n",
            "1900 0.6929158568382263\n",
            "2000 0.6928662657737732\n",
            "2500 0.691940188407898\n",
            "3000 0.5511929988861084\n",
            "3500 0.002833011094480753\n",
            "4000 0.0011025737039744854\n",
            "4500 0.0006645479006692767\n",
            "5000 0.00046999045298434794\n",
            "5500 0.00036121075390838087\n",
            "6000 0.0002922551066149026\n",
            "6500 0.00024469252093695104\n",
            "7000 0.0002100691490340978\n",
            "7500 0.00018377824744675308\n",
            "8000 0.00016316630353685468\n",
            "8500 0.00014654890401288867\n",
            "9000 0.00013292730727698654\n",
            "9500 0.00012157116725575179\n",
            "10000 0.00011192896636202931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSaDRZulEdVX",
        "colab_type": "text"
      },
      "source": [
        "같은 step 수인데도 cost가 훨씬 더 많이 줄어든다는 걸 확인할 수 있다."
      ]
    }
  ]
}
